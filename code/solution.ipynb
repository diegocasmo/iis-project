{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Classification of The Bosphorus Database Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis and classification of the Bosphorus Database Dataset project is to assign one of the six basic emotions (anger, disgust, fear, happy, sadness, and surprise) to a set of labeled facial landmark positions.\n",
    "\n",
    "\n",
    "The Bosphorus Database Dataset emotion samples were provided as text or binary files, with either an ``.lm2`` (2D landmark file with the corresponding labels), ``.lm3`` (3D landmark file with the corresponding labels), or ``.btn`` (coordinate file, both 3D coordinates and corresponding 2D image\n",
    "coordinates) extension. In this notebook we'll use the ``.lm3`` files, which are parsed by the [lm3_parser.py](../parsers/lm3_parser.py) and transformed to a ``.csv`` file for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bosphorus Database Dataset emotion samples were provided as text or binary files, with either an ``.lm2`` (2D landmark file with the corresponding labels), ``.lm3`` (3D landmark file with the corresponding labels), or ``.btn`` (coordinate file, both 3D coordinates and corresponding 2D image\n",
    "coordinates) extension. In this notebook we'll use the ``.lm3`` files, which are parsed by the [lm3_parser.py](../parsers/lm3_parser.py) and transformed to a ``.csv`` file for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(r'../data/lm3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset Format and Number of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of a total of 453 samples and 78 features (26 landmarks, each in the x, y and z coordinates ``26*3=78``) and a label associative to it (79 features in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: \t453\n",
      "Total number of features: \t79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Outer left eyebrow-x</th>\n",
       "      <th>Outer left eyebrow-y</th>\n",
       "      <th>Outer left eyebrow-z</th>\n",
       "      <th>Middle left eyebrow-x</th>\n",
       "      <th>Middle left eyebrow-y</th>\n",
       "      <th>Middle left eyebrow-z</th>\n",
       "      <th>Inner left eyebrow-x</th>\n",
       "      <th>Inner left eyebrow-y</th>\n",
       "      <th>Inner left eyebrow-z</th>\n",
       "      <th>...</th>\n",
       "      <th>Lower lip outer middle-z</th>\n",
       "      <th>Chin middle-x</th>\n",
       "      <th>Chin middle-y</th>\n",
       "      <th>Chin middle-z</th>\n",
       "      <th>Left ear lobe-x</th>\n",
       "      <th>Left ear lobe-y</th>\n",
       "      <th>Left ear lobe-z</th>\n",
       "      <th>Right ear lobe-x</th>\n",
       "      <th>Right ear lobe-y</th>\n",
       "      <th>Right ear lobe-z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANGER</td>\n",
       "      <td>-72.961</td>\n",
       "      <td>-1.725</td>\n",
       "      <td>22.958</td>\n",
       "      <td>-55.678</td>\n",
       "      <td>4.591</td>\n",
       "      <td>38.791</td>\n",
       "      <td>-31.920</td>\n",
       "      <td>-1.929</td>\n",
       "      <td>36.645</td>\n",
       "      <td>...</td>\n",
       "      <td>39.421</td>\n",
       "      <td>-9.840</td>\n",
       "      <td>-112.234</td>\n",
       "      <td>31.313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISGUST</td>\n",
       "      <td>-76.565</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>6.126</td>\n",
       "      <td>-62.086</td>\n",
       "      <td>9.454</td>\n",
       "      <td>24.055</td>\n",
       "      <td>-35.614</td>\n",
       "      <td>2.066</td>\n",
       "      <td>25.073</td>\n",
       "      <td>...</td>\n",
       "      <td>24.378</td>\n",
       "      <td>-13.583</td>\n",
       "      <td>-109.568</td>\n",
       "      <td>19.583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FEAR</td>\n",
       "      <td>-76.163</td>\n",
       "      <td>6.390</td>\n",
       "      <td>10.784</td>\n",
       "      <td>-57.083</td>\n",
       "      <td>16.865</td>\n",
       "      <td>30.162</td>\n",
       "      <td>-33.708</td>\n",
       "      <td>14.082</td>\n",
       "      <td>32.408</td>\n",
       "      <td>...</td>\n",
       "      <td>42.100</td>\n",
       "      <td>-13.020</td>\n",
       "      <td>-110.107</td>\n",
       "      <td>33.429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>-72.140</td>\n",
       "      <td>8.896</td>\n",
       "      <td>9.353</td>\n",
       "      <td>-54.721</td>\n",
       "      <td>22.380</td>\n",
       "      <td>27.474</td>\n",
       "      <td>-29.789</td>\n",
       "      <td>15.802</td>\n",
       "      <td>28.937</td>\n",
       "      <td>...</td>\n",
       "      <td>38.018</td>\n",
       "      <td>-9.030</td>\n",
       "      <td>-97.687</td>\n",
       "      <td>36.058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SADNESS</td>\n",
       "      <td>-73.743</td>\n",
       "      <td>3.099</td>\n",
       "      <td>12.438</td>\n",
       "      <td>-58.607</td>\n",
       "      <td>15.144</td>\n",
       "      <td>29.381</td>\n",
       "      <td>-32.979</td>\n",
       "      <td>11.594</td>\n",
       "      <td>30.048</td>\n",
       "      <td>...</td>\n",
       "      <td>41.452</td>\n",
       "      <td>-10.731</td>\n",
       "      <td>-100.452</td>\n",
       "      <td>35.956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  Outer left eyebrow-x  Outer left eyebrow-y  Outer left eyebrow-z  \\\n",
       "0    ANGER               -72.961                -1.725                22.958   \n",
       "1  DISGUST               -76.565                -0.458                 6.126   \n",
       "2     FEAR               -76.163                 6.390                10.784   \n",
       "3    HAPPY               -72.140                 8.896                 9.353   \n",
       "4  SADNESS               -73.743                 3.099                12.438   \n",
       "\n",
       "   Middle left eyebrow-x  Middle left eyebrow-y  Middle left eyebrow-z  \\\n",
       "0                -55.678                  4.591                 38.791   \n",
       "1                -62.086                  9.454                 24.055   \n",
       "2                -57.083                 16.865                 30.162   \n",
       "3                -54.721                 22.380                 27.474   \n",
       "4                -58.607                 15.144                 29.381   \n",
       "\n",
       "   Inner left eyebrow-x  Inner left eyebrow-y  Inner left eyebrow-z  \\\n",
       "0               -31.920                -1.929                36.645   \n",
       "1               -35.614                 2.066                25.073   \n",
       "2               -33.708                14.082                32.408   \n",
       "3               -29.789                15.802                28.937   \n",
       "4               -32.979                11.594                30.048   \n",
       "\n",
       "         ...         Lower lip outer middle-z  Chin middle-x  Chin middle-y  \\\n",
       "0        ...                           39.421         -9.840       -112.234   \n",
       "1        ...                           24.378        -13.583       -109.568   \n",
       "2        ...                           42.100        -13.020       -110.107   \n",
       "3        ...                           38.018         -9.030        -97.687   \n",
       "4        ...                           41.452        -10.731       -100.452   \n",
       "\n",
       "   Chin middle-z  Left ear lobe-x  Left ear lobe-y  Left ear lobe-z  \\\n",
       "0         31.313              NaN              NaN              NaN   \n",
       "1         19.583              NaN              NaN              NaN   \n",
       "2         33.429              NaN              NaN              NaN   \n",
       "3         36.058              NaN              NaN              NaN   \n",
       "4         35.956              NaN              NaN              NaN   \n",
       "\n",
       "   Right ear lobe-x  Right ear lobe-y  Right ear lobe-z  \n",
       "0               NaN               NaN               NaN  \n",
       "1               NaN               NaN               NaN  \n",
       "2               NaN               NaN               NaN  \n",
       "3               NaN               NaN               NaN  \n",
       "4               NaN               NaN               NaN  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total number of samples: \\t%s' % data_df.shape[0])\n",
    "print('Total number of features: \\t%s' % len(data_df.columns.values))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, some samples do not provide the value of a feature(s), which means that the same feature is also undefined across the 3 dimensions. The [lm3_parser.py](../parsers/lm3_parser.py) handles this use-case by simply setting the value of that feature across the 3 dimensions to ``NaN``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Features Where NaN Is Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features before dropping NaN: \t79\n",
      "Total number of features after dropping NaN: \t58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Outer left eyebrow-x</th>\n",
       "      <th>Outer left eyebrow-y</th>\n",
       "      <th>Outer left eyebrow-z</th>\n",
       "      <th>Middle left eyebrow-x</th>\n",
       "      <th>Middle left eyebrow-y</th>\n",
       "      <th>Middle left eyebrow-z</th>\n",
       "      <th>Inner left eyebrow-x</th>\n",
       "      <th>Inner left eyebrow-y</th>\n",
       "      <th>Inner left eyebrow-z</th>\n",
       "      <th>...</th>\n",
       "      <th>Left mouth corner-z</th>\n",
       "      <th>Upper lip outer middle-x</th>\n",
       "      <th>Upper lip outer middle-y</th>\n",
       "      <th>Upper lip outer middle-z</th>\n",
       "      <th>Right mouth corner-x</th>\n",
       "      <th>Right mouth corner-y</th>\n",
       "      <th>Right mouth corner-z</th>\n",
       "      <th>Lower lip outer middle-x</th>\n",
       "      <th>Lower lip outer middle-y</th>\n",
       "      <th>Lower lip outer middle-z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANGER</td>\n",
       "      <td>-72.961</td>\n",
       "      <td>-1.725</td>\n",
       "      <td>22.958</td>\n",
       "      <td>-55.678</td>\n",
       "      <td>4.591</td>\n",
       "      <td>38.791</td>\n",
       "      <td>-31.920</td>\n",
       "      <td>-1.929</td>\n",
       "      <td>36.645</td>\n",
       "      <td>...</td>\n",
       "      <td>31.654</td>\n",
       "      <td>-11.424</td>\n",
       "      <td>-77.828</td>\n",
       "      <td>44.218</td>\n",
       "      <td>7.849</td>\n",
       "      <td>-78.691</td>\n",
       "      <td>29.980</td>\n",
       "      <td>-9.891</td>\n",
       "      <td>-86.283</td>\n",
       "      <td>39.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISGUST</td>\n",
       "      <td>-76.565</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>6.126</td>\n",
       "      <td>-62.086</td>\n",
       "      <td>9.454</td>\n",
       "      <td>24.055</td>\n",
       "      <td>-35.614</td>\n",
       "      <td>2.066</td>\n",
       "      <td>25.073</td>\n",
       "      <td>...</td>\n",
       "      <td>6.895</td>\n",
       "      <td>-15.941</td>\n",
       "      <td>-66.863</td>\n",
       "      <td>29.844</td>\n",
       "      <td>14.277</td>\n",
       "      <td>-75.678</td>\n",
       "      <td>7.290</td>\n",
       "      <td>-15.118</td>\n",
       "      <td>-83.790</td>\n",
       "      <td>24.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FEAR</td>\n",
       "      <td>-76.163</td>\n",
       "      <td>6.390</td>\n",
       "      <td>10.784</td>\n",
       "      <td>-57.083</td>\n",
       "      <td>16.865</td>\n",
       "      <td>30.162</td>\n",
       "      <td>-33.708</td>\n",
       "      <td>14.082</td>\n",
       "      <td>32.408</td>\n",
       "      <td>...</td>\n",
       "      <td>25.992</td>\n",
       "      <td>-12.448</td>\n",
       "      <td>-60.169</td>\n",
       "      <td>45.930</td>\n",
       "      <td>9.749</td>\n",
       "      <td>-73.120</td>\n",
       "      <td>25.172</td>\n",
       "      <td>-12.362</td>\n",
       "      <td>-84.124</td>\n",
       "      <td>42.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>-72.140</td>\n",
       "      <td>8.896</td>\n",
       "      <td>9.353</td>\n",
       "      <td>-54.721</td>\n",
       "      <td>22.380</td>\n",
       "      <td>27.474</td>\n",
       "      <td>-29.789</td>\n",
       "      <td>15.802</td>\n",
       "      <td>28.937</td>\n",
       "      <td>...</td>\n",
       "      <td>17.507</td>\n",
       "      <td>-7.999</td>\n",
       "      <td>-51.207</td>\n",
       "      <td>40.242</td>\n",
       "      <td>22.196</td>\n",
       "      <td>-56.737</td>\n",
       "      <td>12.908</td>\n",
       "      <td>-7.063</td>\n",
       "      <td>-69.995</td>\n",
       "      <td>38.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SADNESS</td>\n",
       "      <td>-73.743</td>\n",
       "      <td>3.099</td>\n",
       "      <td>12.438</td>\n",
       "      <td>-58.607</td>\n",
       "      <td>15.144</td>\n",
       "      <td>29.381</td>\n",
       "      <td>-32.979</td>\n",
       "      <td>11.594</td>\n",
       "      <td>30.048</td>\n",
       "      <td>...</td>\n",
       "      <td>25.140</td>\n",
       "      <td>-11.017</td>\n",
       "      <td>-62.101</td>\n",
       "      <td>42.409</td>\n",
       "      <td>14.146</td>\n",
       "      <td>-67.281</td>\n",
       "      <td>23.486</td>\n",
       "      <td>-9.479</td>\n",
       "      <td>-74.060</td>\n",
       "      <td>41.452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  Outer left eyebrow-x  Outer left eyebrow-y  Outer left eyebrow-z  \\\n",
       "0    ANGER               -72.961                -1.725                22.958   \n",
       "1  DISGUST               -76.565                -0.458                 6.126   \n",
       "2     FEAR               -76.163                 6.390                10.784   \n",
       "3    HAPPY               -72.140                 8.896                 9.353   \n",
       "4  SADNESS               -73.743                 3.099                12.438   \n",
       "\n",
       "   Middle left eyebrow-x  Middle left eyebrow-y  Middle left eyebrow-z  \\\n",
       "0                -55.678                  4.591                 38.791   \n",
       "1                -62.086                  9.454                 24.055   \n",
       "2                -57.083                 16.865                 30.162   \n",
       "3                -54.721                 22.380                 27.474   \n",
       "4                -58.607                 15.144                 29.381   \n",
       "\n",
       "   Inner left eyebrow-x  Inner left eyebrow-y  Inner left eyebrow-z  \\\n",
       "0               -31.920                -1.929                36.645   \n",
       "1               -35.614                 2.066                25.073   \n",
       "2               -33.708                14.082                32.408   \n",
       "3               -29.789                15.802                28.937   \n",
       "4               -32.979                11.594                30.048   \n",
       "\n",
       "             ...             Left mouth corner-z  Upper lip outer middle-x  \\\n",
       "0            ...                          31.654                   -11.424   \n",
       "1            ...                           6.895                   -15.941   \n",
       "2            ...                          25.992                   -12.448   \n",
       "3            ...                          17.507                    -7.999   \n",
       "4            ...                          25.140                   -11.017   \n",
       "\n",
       "   Upper lip outer middle-y  Upper lip outer middle-z  Right mouth corner-x  \\\n",
       "0                   -77.828                    44.218                 7.849   \n",
       "1                   -66.863                    29.844                14.277   \n",
       "2                   -60.169                    45.930                 9.749   \n",
       "3                   -51.207                    40.242                22.196   \n",
       "4                   -62.101                    42.409                14.146   \n",
       "\n",
       "   Right mouth corner-y  Right mouth corner-z  Lower lip outer middle-x  \\\n",
       "0               -78.691                29.980                    -9.891   \n",
       "1               -75.678                 7.290                   -15.118   \n",
       "2               -73.120                25.172                   -12.362   \n",
       "3               -56.737                12.908                    -7.063   \n",
       "4               -67.281                23.486                    -9.479   \n",
       "\n",
       "   Lower lip outer middle-y  Lower lip outer middle-z  \n",
       "0                   -86.283                    39.421  \n",
       "1                   -83.790                    24.378  \n",
       "2                   -84.124                    42.100  \n",
       "3                   -69.995                    38.018  \n",
       "4                   -74.060                    41.452  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total number of features before dropping NaN: \\t%s' % len(data_df.columns.values))\n",
    "data_df = data_df.dropna(axis=1, how='any')\n",
    "print('Total number of features after dropping NaN: \\t%s' % len(data_df.columns.values))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization refers to the process of standardizing the values of independent features of a dataset. Since many of the machine learning techniques use distance to compute the difference between two or more distinct samples, a feature within these samples that has a broad range of values will dominate the process. In order to avoid this, the range of all features are normalized so that each feature contributes approximately proportionately to the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Separate features from their labels\n",
    "features = [x != 'Label' for x in data_df.columns.values]\n",
    "all_values = data_df.loc[:, features].values\n",
    "all_labels = data_df.loc[:,['Label']].values\n",
    "\n",
    "# Scale values (mean = 0 and variance = 1)\n",
    "all_values =  normalize(all_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Furthermore, as the number of features of a dataset increases,  the concept of distance becomes less meaningful (which once again, does not help the set of machine learning techniques that rely on distance computations). This phenomenon is known as the curse of dimensionality and can be alleviated by using dimensionality reduction techniques.\n",
    "\n",
    "#### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a statistical method that can be used to reduce the dimensionality of a dataset. It works by first finding the principal component that accounts for as much of the variance in the data features as possible. Next, each succeeding component in turn is selected such that it has the highest variance possible under the restriction that it is orthogonal to the previous principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANGER</td>\n",
       "      <td>-0.697793</td>\n",
       "      <td>-1.709458</td>\n",
       "      <td>-0.889387</td>\n",
       "      <td>0.155860</td>\n",
       "      <td>-0.301027</td>\n",
       "      <td>0.945531</td>\n",
       "      <td>0.131846</td>\n",
       "      <td>-1.134745</td>\n",
       "      <td>0.025240</td>\n",
       "      <td>-1.389868</td>\n",
       "      <td>0.890627</td>\n",
       "      <td>-0.206171</td>\n",
       "      <td>-0.242079</td>\n",
       "      <td>0.287009</td>\n",
       "      <td>-1.094219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISGUST</td>\n",
       "      <td>-0.448693</td>\n",
       "      <td>-1.802231</td>\n",
       "      <td>-0.821523</td>\n",
       "      <td>0.719338</td>\n",
       "      <td>0.872276</td>\n",
       "      <td>-0.232737</td>\n",
       "      <td>1.047300</td>\n",
       "      <td>0.174249</td>\n",
       "      <td>-0.077335</td>\n",
       "      <td>-1.358734</td>\n",
       "      <td>0.565688</td>\n",
       "      <td>0.932360</td>\n",
       "      <td>-0.216993</td>\n",
       "      <td>0.273641</td>\n",
       "      <td>-1.966553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FEAR</td>\n",
       "      <td>-0.628858</td>\n",
       "      <td>-1.547821</td>\n",
       "      <td>-0.304395</td>\n",
       "      <td>0.960450</td>\n",
       "      <td>-0.945108</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.382293</td>\n",
       "      <td>-0.573876</td>\n",
       "      <td>-0.893066</td>\n",
       "      <td>-1.353104</td>\n",
       "      <td>-1.104153</td>\n",
       "      <td>-0.623738</td>\n",
       "      <td>0.560742</td>\n",
       "      <td>0.886852</td>\n",
       "      <td>-1.091245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>-0.644286</td>\n",
       "      <td>-1.153386</td>\n",
       "      <td>-0.218719</td>\n",
       "      <td>1.237920</td>\n",
       "      <td>-0.024519</td>\n",
       "      <td>-2.071001</td>\n",
       "      <td>-0.387038</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>-0.797599</td>\n",
       "      <td>-1.275211</td>\n",
       "      <td>1.047335</td>\n",
       "      <td>0.446685</td>\n",
       "      <td>-0.702892</td>\n",
       "      <td>0.238254</td>\n",
       "      <td>0.433060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SADNESS</td>\n",
       "      <td>-0.642914</td>\n",
       "      <td>-1.516560</td>\n",
       "      <td>-0.442015</td>\n",
       "      <td>0.870242</td>\n",
       "      <td>-0.728814</td>\n",
       "      <td>-0.468546</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>-1.073784</td>\n",
       "      <td>-0.825473</td>\n",
       "      <td>-1.461662</td>\n",
       "      <td>0.205818</td>\n",
       "      <td>-0.280382</td>\n",
       "      <td>0.963432</td>\n",
       "      <td>0.498373</td>\n",
       "      <td>-1.492330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label         0         1         2         3         4         5  \\\n",
       "0    ANGER -0.697793 -1.709458 -0.889387  0.155860 -0.301027  0.945531   \n",
       "1  DISGUST -0.448693 -1.802231 -0.821523  0.719338  0.872276 -0.232737   \n",
       "2     FEAR -0.628858 -1.547821 -0.304395  0.960450 -0.945108  0.958008   \n",
       "3    HAPPY -0.644286 -1.153386 -0.218719  1.237920 -0.024519 -2.071001   \n",
       "4  SADNESS -0.642914 -1.516560 -0.442015  0.870242 -0.728814 -0.468546   \n",
       "\n",
       "          6         7         8         9        10        11        12  \\\n",
       "0  0.131846 -1.134745  0.025240 -1.389868  0.890627 -0.206171 -0.242079   \n",
       "1  1.047300  0.174249 -0.077335 -1.358734  0.565688  0.932360 -0.216993   \n",
       "2  0.382293 -0.573876 -0.893066 -1.353104 -1.104153 -0.623738  0.560742   \n",
       "3 -0.387038  0.543478 -0.797599 -1.275211  1.047335  0.446685 -0.702892   \n",
       "4  0.020666 -1.073784 -0.825473 -1.461662  0.205818 -0.280382  0.963432   \n",
       "\n",
       "         13        14  \n",
       "0  0.287009 -1.094219  \n",
       "1  0.273641 -1.966553  \n",
       "2  0.886852 -1.091245  \n",
       "3  0.238254  0.433060  \n",
       "4  0.498373 -1.492330  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use PCA to reduce dimensionality\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 15, whiten = True)\n",
    "principal_components = pca.fit_transform(all_values)\n",
    "principal_components_df = pd.DataFrame(data = principal_components)\n",
    "\n",
    "# export model\n",
    "joblib.dump(pca, 'pca-model.pkl') \n",
    "\n",
    "# Add the label to the dataframe with the principal components\n",
    "pca_df = pd.concat([data_df[['Label']], principal_components_df], axis = 1)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the PCA 2d projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ae17ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "x_label = pca_df.columns.values[1]\n",
    "y_label = pca_df.columns.values[2]\n",
    "ax.set_xlabel(x_label, fontsize = 12)\n",
    "ax.set_ylabel(y_label, fontsize = 12)\n",
    "ax.set_title('3 Components PCA', fontsize = 15)\n",
    "\n",
    "labels = np.unique(data_df.loc[:,['Label']].values)\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "for label, color in zip(labels, colors):\n",
    "    indexes_to_keep = pca_df['Label'] == label\n",
    "    ax.scatter(\n",
    "        pca_df.loc[indexes_to_keep, x_label],\n",
    "        pca_df.loc[indexes_to_keep, y_label],\n",
    "        c = color,\n",
    "        s = 50\n",
    "    )\n",
    "ax.legend(labels)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-Distributed Stochastic Neighbor Embedding  (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is another technique that can be used to perform dimensionality reduction. It embeds high-dimensional data in a low-dimensional space by modeling each high-dimensional object by a two- or three-dimensional point such that objects that are alike are modeled by nearby points and dissimilar objects are modeled otherwise with a high probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components = 15, init = 'pca', random_state = 0, method='exact')\n",
    "tsne_components = tsne.fit_transform(all_values)\n",
    "tsne_components_df = pd.DataFrame(data = tsne_components)\n",
    "\n",
    "# Add the label to the dataframe with the t-SNE components\n",
    "tsne_df = pd.concat([data_df[['Label']], tsne_components_df], axis = 1)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the t-SNE 2d projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_components[:,0], tsne_components[:,1], c=colors, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance refers to the phenomenon where some classes (labels) of a dataset have more samples than others. This is a problem because the machine learning algorithms will tend to focus on the classification of the samples that are overrepresented while ignoring or misclassifying the underrepresented samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = tsne_df.shape\n",
    "print(tsne_df.groupby('Label').count()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we can see the dataset has a class imbalance problem. The emotion \"HAPPY\" has 106 samples, while \"SADNESS\" has only 66. We'll fix this issue by carefully selecting the number of samples we'll use for training the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the holdout method to create a training and test set with an ~80/20 split. As mentioned above, the dataset suffers from a class imbalance problem, and to solve this we'll randomly select the same number of samples per class to add to the training set (60 per class, 360 samples = ~80% of the dataset), and the remaining will be added to the test set (no sample is allowed to be in both sets at the same time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(df):\n",
    "    '''\n",
    "    Split data into training and test sets\n",
    "    '''\n",
    "    # Shuffle data frame\n",
    "    df = df.sample(frac=1)\n",
    "    \n",
    "    # Select same number of samples per class for train set, remaining go to test set\n",
    "    num_of_train_inputs = int(rows * 0.8 / 6)\n",
    "    train_df, test_df = (pd.DataFrame(columns=df.columns.values), pd.DataFrame(columns=df.columns.values))\n",
    "    labels = np.unique(df.loc[:,['Label']].values)\n",
    "    for label in labels:\n",
    "        train_df = train_df.append(df.loc[df['Label'] == label][0:num_of_train_inputs], ignore_index=True)\n",
    "        test_df = test_df.append(df.loc[df['Label'] == label][num_of_train_inputs:], ignore_index=True)\n",
    "\n",
    "    # Shuffle data frames (because they were appended in an orderly per label fashion)\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    test_df = test_df.sample(frac=1)\n",
    "    \n",
    "    # Split train and test datasets into labels/features\n",
    "    train_values   = train_df.iloc[:,1:].values\n",
    "    train_labels = train_df.iloc[:,:1].values.ravel()\n",
    "\n",
    "    test_values   = test_df.iloc[:,1:].values\n",
    "    test_labels = test_df.iloc[:,:1].values.ravel()\n",
    "    \n",
    "    return (train_values, train_labels, test_values, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with the different machine learning methods which we'll use for classification, let's create a helper method which renders a confusion matrix of a specified model prediction output. A confusion matrix is a table often used to analyze the performance of a classifier on samples for which the true values are known (we'll use it to analyze the performance of the machine learning methods in the test set). Each row in the table represents the instances in an actual class while each column instances in a predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    '''\n",
    "    Plot confusion matrix of the specified accuracies and labels\n",
    "    '''\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Draw ticks\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    \n",
    "    # Normalize\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.2f}; misclass={:0.2f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kNN classifier simply classifies samples by a majority vote of its neighbors, with the object being assigned to the class most common among its ``k`` nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters:\n",
    "- ``n_neighbors``: Number of neighbors to use to determine the label of a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a kNN model using the PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get train/test values and labels using the PCA components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(pca_df)\n",
    "\n",
    "kNNClassifier = neighbors.KNeighborsClassifier(n_neighbors=15).fit(train_values, train_labels)\n",
    "predicted_labels = kNNClassifier.predict(test_values)\n",
    "knn_pca_acc = accuracy_score(test_labels, predicted_labels)\n",
    "knn_pca_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(knn_pca_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a kNN model using the t-SNE components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test values and labels using the t-SNE components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(tsne_df)\n",
    "\n",
    "kNNClassifier = neighbors.KNeighborsClassifier(n_neighbors=15).fit(train_values, train_labels)\n",
    "predicted_labels = kNNClassifier.predict(test_values)\n",
    "knn_tsne_acc = accuracy_score(test_labels, predicted_labels)\n",
    "knn_tsne_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(knn_tsne_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVM is a supervised learning algorithm which creates a model that represents the input samples as points in space, in such a way that samples of different classes are divided by a gap that is as wide as possible. New samples are then mapped into that same space and predicted to belong to a class based on which side of the gap they fall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SVM model using the PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Get train/test values and labels using the PCA components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(pca_df)\n",
    "\n",
    "clf = LinearSVC()\n",
    "svm = CalibratedClassifierCV(clf)\n",
    "svm.fit(train_values, train_labels)\n",
    "\n",
    "# export model\n",
    "joblib.dump(svm, 'svm-model.pkl') \n",
    "\n",
    "predicted_labels = svm.predict(test_values)\n",
    "svm_pca_acc = accuracy_score(test_labels, predicted_labels)\n",
    "svm_pca_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(svm_pca_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SVM model using the t-SNE components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test values and labels using the t-SNE components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(tsne_df)\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(train_values, train_labels)\n",
    "predicted_labels = svm.predict(test_values)\n",
    "svm_tsne_acc = accuracy_score(test_labels, predicted_labels)\n",
    "svm_tsne_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(svm_tsne_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLP is a feed-forward artificial neural network that uses supervised learning with backpropagation for training the network (update its weights) to essentially map a weighted input to a label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters:\n",
    "- ``solver``: The solver for weight optimization (``adam`` is a stochastic gradient-based optimizer).\n",
    "- ``activation``: Activation function for the hidden layer, ``f(x) = x``.\n",
    "- ``hidden_layer_sizes``: Number of neurons per hidden layer (a single layer with 18 neurons).\n",
    "- ``max_iter``: The solver will iterate until convergence or this number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MLP model using the PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Get train/test values and labels using the PCA components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(pca_df)\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', activation='logistic', hidden_layer_sizes=(5), max_iter=2000)\n",
    "mlp.fit(train_values, train_labels)\n",
    "\n",
    "# export model\n",
    "joblib.dump(mlp, 'mlp-model.pkl') \n",
    "\n",
    "predicted_labels = mlp.predict(test_values)\n",
    "mlp_pca_acc = accuracy_score(test_labels, predicted_labels)\n",
    "mlp_pca_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(mlp_pca_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MLP model using the t-SNE components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test values and labels using the t-SNE components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(tsne_df)\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', activation='logistic', hidden_layer_sizes=(5,), max_iter=2000)\n",
    "mlp.fit(train_values, train_labels)\n",
    "predicted_labels = mlp.predict(test_values)\n",
    "mlp_tsne_acc = accuracy_score(test_labels, predicted_labels)\n",
    "mlp_tsne_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(mlp_tsne_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is an ensemble learning method for classification that works by creating multiple decision trees during training and predicting the class that is the mode of the classes of the individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters:\n",
    "- ``n_estimators``: The number of trees in the forest.\n",
    "- ``criterion``: The function to measure the quality of a split.\n",
    "- ``max_depth``: The maximum depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Random Forest model using the PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Get train/test values and labels using the PCA components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(pca_df)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=6)\n",
    "random_forest.fit(train_values, train_labels)\n",
    "predicted_labels = random_forest.predict(test_values)\n",
    "random_forest_pca_acc = accuracy_score(test_labels, predicted_labels)\n",
    "random_forest_pca_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(random_forest_pca_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Random Forest model using the t-SNE components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test values and labels using the t-SNE components\n",
    "train_values, train_labels, test_values, test_labels = get_train_test_split(tsne_df)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=6)\n",
    "random_forest.fit(train_values, train_labels)\n",
    "predicted_labels = random_forest.predict(test_values)\n",
    "random_forest_tsne_acc = accuracy_score(test_labels, predicted_labels)\n",
    "random_forest_tsne_cm = confusion_matrix(test_labels, predicted_labels)\n",
    "plot_confusion_matrix(random_forest_tsne_cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have analyzed the Bosphorus Database Dataset, performed data cleansing, normalization, dimensionality reduction (PCA and t-SNE), class imbalance analysis, holdout split, and finally used four different machine learning methods (kNN, SVM, MLP, and Random Forest) to classify the emotions in the dataset. The SVM and MLP models using the principal components of the PCA reduction have the best accuracy, usually around ~70-80%, while the kNN and Random Forest have an accuracy of ~60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_models = {\n",
    "    'kNN with PCA': knn_pca_acc,\n",
    "    'SVM with PCA': svm_pca_acc,\n",
    "    'MLP with PCA': mlp_pca_acc,\n",
    "    'Random Forest with PCA': random_forest_pca_acc\n",
    "}\n",
    "\n",
    "for model, acc in pca_models.items():\n",
    "    print('%s classifier accuracy: \\t%.02f%%' % (model, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_models = {\n",
    "    'kNN with TSNE': knn_tsne_acc,\n",
    "    'SVM with TSNE': svm_tsne_acc,\n",
    "    'MLP with TSNE': mlp_tsne_acc,\n",
    "    'Random Forest with TSNE': random_forest_tsne_acc\n",
    "}\n",
    "    \n",
    "for model, acc in tsne_models.items():\n",
    "    print('%s classifier accuracy: \\t%.02f%%' % (model, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
