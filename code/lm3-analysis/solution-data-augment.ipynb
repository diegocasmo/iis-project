{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of features: 79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Outer left eyebrow-x</th>\n",
       "      <th>Outer left eyebrow-y</th>\n",
       "      <th>Outer left eyebrow-z</th>\n",
       "      <th>Middle left eyebrow-x</th>\n",
       "      <th>Middle left eyebrow-y</th>\n",
       "      <th>Middle left eyebrow-z</th>\n",
       "      <th>Inner left eyebrow-x</th>\n",
       "      <th>Inner left eyebrow-y</th>\n",
       "      <th>Inner left eyebrow-z</th>\n",
       "      <th>...</th>\n",
       "      <th>Lower lip outer middle-z</th>\n",
       "      <th>Chin middle-x</th>\n",
       "      <th>Chin middle-y</th>\n",
       "      <th>Chin middle-z</th>\n",
       "      <th>Left ear lobe-x</th>\n",
       "      <th>Left ear lobe-y</th>\n",
       "      <th>Left ear lobe-z</th>\n",
       "      <th>Right ear lobe-x</th>\n",
       "      <th>Right ear lobe-y</th>\n",
       "      <th>Right ear lobe-z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANGER</td>\n",
       "      <td>-72.961</td>\n",
       "      <td>-1.725</td>\n",
       "      <td>22.958</td>\n",
       "      <td>-55.678</td>\n",
       "      <td>4.591</td>\n",
       "      <td>38.791</td>\n",
       "      <td>-31.920</td>\n",
       "      <td>-1.929</td>\n",
       "      <td>36.645</td>\n",
       "      <td>...</td>\n",
       "      <td>39.421</td>\n",
       "      <td>-9.840</td>\n",
       "      <td>-112.234</td>\n",
       "      <td>31.313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISGUST</td>\n",
       "      <td>-76.565</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>6.126</td>\n",
       "      <td>-62.086</td>\n",
       "      <td>9.454</td>\n",
       "      <td>24.055</td>\n",
       "      <td>-35.614</td>\n",
       "      <td>2.066</td>\n",
       "      <td>25.073</td>\n",
       "      <td>...</td>\n",
       "      <td>24.378</td>\n",
       "      <td>-13.583</td>\n",
       "      <td>-109.568</td>\n",
       "      <td>19.583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FEAR</td>\n",
       "      <td>-76.163</td>\n",
       "      <td>6.390</td>\n",
       "      <td>10.784</td>\n",
       "      <td>-57.083</td>\n",
       "      <td>16.865</td>\n",
       "      <td>30.162</td>\n",
       "      <td>-33.708</td>\n",
       "      <td>14.082</td>\n",
       "      <td>32.408</td>\n",
       "      <td>...</td>\n",
       "      <td>42.100</td>\n",
       "      <td>-13.020</td>\n",
       "      <td>-110.107</td>\n",
       "      <td>33.429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAPPY</td>\n",
       "      <td>-72.140</td>\n",
       "      <td>8.896</td>\n",
       "      <td>9.353</td>\n",
       "      <td>-54.721</td>\n",
       "      <td>22.380</td>\n",
       "      <td>27.474</td>\n",
       "      <td>-29.789</td>\n",
       "      <td>15.802</td>\n",
       "      <td>28.937</td>\n",
       "      <td>...</td>\n",
       "      <td>38.018</td>\n",
       "      <td>-9.030</td>\n",
       "      <td>-97.687</td>\n",
       "      <td>36.058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SADNESS</td>\n",
       "      <td>-73.743</td>\n",
       "      <td>3.099</td>\n",
       "      <td>12.438</td>\n",
       "      <td>-58.607</td>\n",
       "      <td>15.144</td>\n",
       "      <td>29.381</td>\n",
       "      <td>-32.979</td>\n",
       "      <td>11.594</td>\n",
       "      <td>30.048</td>\n",
       "      <td>...</td>\n",
       "      <td>41.452</td>\n",
       "      <td>-10.731</td>\n",
       "      <td>-100.452</td>\n",
       "      <td>35.956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  Outer left eyebrow-x  Outer left eyebrow-y  Outer left eyebrow-z  \\\n",
       "0    ANGER               -72.961                -1.725                22.958   \n",
       "1  DISGUST               -76.565                -0.458                 6.126   \n",
       "2     FEAR               -76.163                 6.390                10.784   \n",
       "3    HAPPY               -72.140                 8.896                 9.353   \n",
       "4  SADNESS               -73.743                 3.099                12.438   \n",
       "\n",
       "   Middle left eyebrow-x  Middle left eyebrow-y  Middle left eyebrow-z  \\\n",
       "0                -55.678                  4.591                 38.791   \n",
       "1                -62.086                  9.454                 24.055   \n",
       "2                -57.083                 16.865                 30.162   \n",
       "3                -54.721                 22.380                 27.474   \n",
       "4                -58.607                 15.144                 29.381   \n",
       "\n",
       "   Inner left eyebrow-x  Inner left eyebrow-y  Inner left eyebrow-z  \\\n",
       "0               -31.920                -1.929                36.645   \n",
       "1               -35.614                 2.066                25.073   \n",
       "2               -33.708                14.082                32.408   \n",
       "3               -29.789                15.802                28.937   \n",
       "4               -32.979                11.594                30.048   \n",
       "\n",
       "         ...         Lower lip outer middle-z  Chin middle-x  Chin middle-y  \\\n",
       "0        ...                           39.421         -9.840       -112.234   \n",
       "1        ...                           24.378        -13.583       -109.568   \n",
       "2        ...                           42.100        -13.020       -110.107   \n",
       "3        ...                           38.018         -9.030        -97.687   \n",
       "4        ...                           41.452        -10.731       -100.452   \n",
       "\n",
       "   Chin middle-z  Left ear lobe-x  Left ear lobe-y  Left ear lobe-z  \\\n",
       "0         31.313              NaN              NaN              NaN   \n",
       "1         19.583              NaN              NaN              NaN   \n",
       "2         33.429              NaN              NaN              NaN   \n",
       "3         36.058              NaN              NaN              NaN   \n",
       "4         35.956              NaN              NaN              NaN   \n",
       "\n",
       "   Right ear lobe-x  Right ear lobe-y  Right ear lobe-z  \n",
       "0               NaN               NaN               NaN  \n",
       "1               NaN               NaN               NaN  \n",
       "2               NaN               NaN               NaN  \n",
       "3               NaN               NaN               NaN  \n",
       "4               NaN               NaN               NaN  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load original dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r'../../data/lm3.csv'\n",
    "data_df = pd.read_csv(file_path)\n",
    "print('Num of features: %s' % len(data_df.columns.values))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "def apply_rand(x):\n",
    "    '''\n",
    "    Applies some noise to a datacell, this is probably best kept <0.1\n",
    "    '''\n",
    "    if type(x) is str:\n",
    "        return x\n",
    "    return x + uniform(-0.05, 0.05)\n",
    "\n",
    "def augment_data(dataframe, n=2):\n",
    "    '''\n",
    "    Mulitplies the size of the passed dataframe by n\n",
    "    '''\n",
    "    if n < 1: \n",
    "        raise ValueError('n must be >= 1')\n",
    "    modded = dataframe.copy() \n",
    "    for i in range(1,n):\n",
    "        frames = [dataframe, modded.applymap(apply_rand)]\n",
    "        dataframe = pd.concat(frames, axis = 0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(df, column_names):\n",
    "    '''\n",
    "    Split data into training and test sets\n",
    "    '''\n",
    "    # Shuffle data frame\n",
    "    df = df.sample(frac=1)\n",
    "    # Select same num per class, remaining go to test set\n",
    "    rows, _ = df.shape\n",
    "    num_of_inputs = int(rows * 0.8 / 6) # this was formerly the magic number 63.\n",
    "    cols = ['Label'] +  column_names\n",
    "    train_df, test_df = (pd.DataFrame(columns=cols), pd.DataFrame(columns=cols))\n",
    "    for x in get_all_emotions() :\n",
    "        train_df = train_df.append(df.loc[df['Label'] == x][0:num_of_inputs], ignore_index=True)\n",
    "        test_df = test_df.append(df.loc[df['Label'] == x][num_of_inputs:], ignore_index=True)\n",
    "\n",
    "    # Shuffle data frames\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    test_df = test_df.sample(frac=1)\n",
    "    \n",
    "    return (train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_data, reduce_features\n",
    "\n",
    "column_names, principal_labels_df = reduce_features(preprocess_data(data_df))\n",
    "# rows, _ = principal_labels_df.shape\n",
    "# print('before %d' % rows)\n",
    "# principal_labels_df = augment_data(principal_labels_df, 8)\n",
    "# rows, _ = principal_labels_df.shape\n",
    "# print('after: %d' % rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Data---\n",
      "train: 1440\n",
      "test:  93\n",
      "--------------------Result--------------------\n",
      "deep MLP classifier accuracy: 0.51%\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for classification\n",
    "from constants import get_all_emotions\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "verbose = False\n",
    "runs = 5\n",
    "mlp_scores = []\n",
    "for index in range(runs):\n",
    "    train_df, test_df = get_train_test_split(principal_labels_df, column_names)\n",
    "\n",
    "    # augment train_df, increase size\n",
    "    train_df = augment_data(train_df, 4)\n",
    "    \n",
    "    # Split train and test labels/data\n",
    "    train_data   = train_df.iloc[:,1:].values\n",
    "    train_labels = train_df.iloc[:,:1].values.ravel()\n",
    "\n",
    "    test_data   = test_df.iloc[:,1:].values\n",
    "    test_labels = test_df.iloc[:,:1].values.ravel()\n",
    "    \n",
    "    # Just print once\n",
    "    if index == 0 and verbose:\n",
    "        # Take a look at the labels distribution\n",
    "        print('--------------------Training--------------------')\n",
    "        rows, cols = train_df.shape\n",
    "        print(train_df.groupby('Label').count())\n",
    "        print('Total number of inputs: %s' % rows)\n",
    "\n",
    "        print('--------------------Testing--------------------')\n",
    "        rows, cols = test_df.shape\n",
    "        print(test_df.groupby('Label').count())\n",
    "        print('Total number of inputs: %s' % rows)\n",
    "        \n",
    "    # MLP classifier to predict\n",
    "    mlp = MLPClassifier(solver='adam', activation='relu', \\\n",
    "                        hidden_layer_sizes=(10,10,10), max_iter=2000)\n",
    "    mlp.fit(train_data, train_labels)\n",
    "    predicted_labels = mlp.predict(test_data)\n",
    "    acc_mlp = accuracy_score(test_labels, predicted_labels)\n",
    "    mlp_scores.append(acc_mlp)\n",
    "\n",
    "print('---Data---')\n",
    "print('train: %d\\ntest:  %d' % (len(train_data), len(test_data)))\n",
    "    \n",
    "print('--------------------Result--------------------')\n",
    "print('deep MLP classifier accuracy: %.02f%%' % (sum(mlp_scores) / runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "- 100 runs with this data took quite some time, you get about the same result with 10.\n",
    "- By augmenting the data we can create tons more samples for supervised training. It also opens up the possibility for some severe overfitting. Don't make the dataset too large similarly don't make the noise we're applying too large either. \n",
    "- It appears the larger the dataset the better the accuracy, that is enought to be suspect about this method. Augmenting data like this should definitely not be seen as a silver bullet, especially doing it like this where we already have some tagged features. For example, if we were doing some classification on cats and we were given a dataset of [tabby cat](https://en.wikipedia.org/wiki/Tabby_cat) landmarks, data augmentation wouldn't help at all for making a generalized model for all cats, just tabbey's. Supplying another cat breed to the dataset would not do much help. Back to our case, we only have maybe a dozen(?) distinct faces, can we create every face in existence from these faces? Probably definitely not. This method is flawed and this notebook is a demonstration in poor data-practices at best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
